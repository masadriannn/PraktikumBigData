{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b199adb7",
   "metadata": {},
   "source": [
    "# Setup Jupyter Notebook for Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17323048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  Row(name='Antony', age=27, height=168)\n",
      "a:  Row(name='Antony', age=27, height=168)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "a = Row(name='Antony', age=27, height=168)\n",
    "print(\"a: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af96c929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-7Q43GQ7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-7Q43GQ7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af066ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|   nama|umur|tinggi|\n",
      "+-------+----+------+\n",
      "| Adrian|  21|   165|\n",
      "|    Dwi|  21|   165|\n",
      "|Adinata|  21|   165|\n",
      "+-------+----+------+\n",
      "\n",
      "+-------+----+------+\n",
      "|   nama|umur|tinggi|\n",
      "+-------+----+------+\n",
      "| Adrian|  21|   165|\n",
      "|    Dwi|  21|   165|\n",
      "|Adinata|  21|   165|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sc.parallelize([ \\\n",
    "    Row(nama='Adrian', umur=21, tinggi=165), \\\n",
    "    Row(nama='Dwi', umur=21, tinggi=165), \\\n",
    "    Row(nama='Adinata', umur=21, tinggi=165)]).toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ba329",
   "metadata": {},
   "source": [
    "# Introduction to DataFrame dan SQL Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97153cc7",
   "metadata": {},
   "source": [
    "## Membaca Data File ke DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74f7aed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+----------------+------+-------+\n",
      "|      Date| Time|      City|            Item| Total|Payment|\n",
      "+----------+-----+----------+----------------+------+-------+\n",
      "|2012-01-01|09:00|  San Jose|  Men's Clothing|214.05|   Amex|\n",
      "|2012-01-01|09:00|Fort Worth|Women's Clothing|153.57|   Visa|\n",
      "|2012-01-01|09:00| San Diego|           Music| 66.08|   Cash|\n",
      "+----------+-----+----------+----------------+------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+-----+----------+----------------+------+-------+\n",
      "|      Date| Time|      City|            Item| Total|Payment|\n",
      "+----------+-----+----------+----------------+------+-------+\n",
      "|2012-01-01|09:00|  San Jose|  Men's Clothing|214.05|   Amex|\n",
      "|2012-01-01|09:00|Fort Worth|Women's Clothing|153.57|   Visa|\n",
      "|2012-01-01|09:00| San Diego|           Music| 66.08|   Cash|\n",
      "+----------+-----+----------+----------------+------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mengimpor modul yang dibutuhkan\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#buat sesi agar dapat mengakses semua API Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Introdution to Spark DataFrame\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#tentukan skema data untuk file yang ingin kita baca\n",
    "purchaseSchema = StructType([\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Time\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Item\", StringType(), True),\n",
    "    StructField(\"Total\", FloatType(), True),\n",
    "    StructField(\"Payment\", StringType(), True),\n",
    "])    \n",
    "\n",
    "#baca file csv dengan skema yang kita tentukan ke dalam Spark DataFrame, dan gunakan pembatas \"tab\"\n",
    "purchaseDataframe = spark.read.csv(\n",
    "    \"dataset/purchases.csv\", \n",
    "    header=True, schema=purchaseSchema, sep=\"\\t\")\n",
    "#tampilkan 3 baris DataFrame\n",
    "purchaseDataframe.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3facf093",
   "metadata": {},
   "source": [
    "## Menghitung jumlah baris, mencetak skema kerangka data, dan mencetak statistik data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69bf43ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows:  4138476\n",
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Total: float (nullable = true)\n",
      " |-- Payment: string (nullable = true)\n",
      "\n",
      "number of rows:  4138476\n",
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Total: float (nullable = true)\n",
      " |-- Payment: string (nullable = true)\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|             Total|\n",
      "+-------+------------------+\n",
      "|  count|           4138476|\n",
      "|   mean|249.96108549398525|\n",
      "| stddev| 144.3174111542959|\n",
      "|    min|               0.0|\n",
      "|    max|            499.99|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|             Total|\n",
      "+-------+------------------+\n",
      "|  count|           4138476|\n",
      "|   mean|249.96108549398525|\n",
      "| stddev| 144.3174111542959|\n",
      "|    min|               0.0|\n",
      "|    max|            499.99|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hitung jumlah baris dataFrame\n",
    "num_rows = purchaseDataframe.count()\n",
    "print(\"number of rows: \", num_rows)\n",
    "#tampilkan skema dataFrame\n",
    "purchaseDataframe.printSchema()\n",
    "#tampilkan statistik dari data yang kita inginkan\n",
    "purchaseDataframe.describe('Total').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47950072",
   "metadata": {},
   "source": [
    "## Membuat dataFrame baru dari subset dataFrame yang ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5766d526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      City| Total|\n",
      "+----------+------+\n",
      "|  San Jose|214.05|\n",
      "|Fort Worth|153.57|\n",
      "| San Diego| 66.08|\n",
      "+----------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Total: float (nullable = true)\n",
      "\n",
      "+----------+------+\n",
      "|      City| Total|\n",
      "+----------+------+\n",
      "|  San Jose|214.05|\n",
      "|Fort Worth|153.57|\n",
      "| San Diego| 66.08|\n",
      "+----------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Total: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#buat dataFrame baru dari kolom \"Kota\" dan \"Total\"\n",
    "newDataframe = purchaseDataframe.select(purchaseDataframe['City'], \n",
    "                                              purchaseDataframe['Total'])\n",
    "newDataframe.show(3); #menampilkan 3 baris DataFrame baru\n",
    "newDataframe.printSchema() #print skema dari DataFrame baru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe354f3",
   "metadata": {},
   "source": [
    "## Menambahkan nilai konstan ke setiap data baris di kolom tertentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82399225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|      City|(Total + 10)|\n",
      "+----------+------------+\n",
      "|  San Jose|      224.05|\n",
      "|Fort Worth|      163.57|\n",
      "| San Diego|       76.08|\n",
      "+----------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+------------+\n",
      "|      City|(Total + 10)|\n",
      "+----------+------------+\n",
      "|  San Jose|      224.05|\n",
      "|Fort Worth|      163.57|\n",
      "| San Diego|       76.08|\n",
      "+----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#menambahkan nilai konstan 10 untuk setiap data baris di kolom \"Total\"\n",
    "purchaseDataframe.select(purchaseDataframe['City'],\n",
    "                         purchaseDataframe['Total']+10).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7f5ed5",
   "metadata": {},
   "source": [
    "## Memfilter dataFrame menggunakan kondisi yang ditentukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1dadabfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------------------+------+----------+\n",
      "|      Date| Time|      City|               Item| Total|   Payment|\n",
      "+----------+-----+----------+-------------------+------+----------+\n",
      "|2012-01-01|09:00|  San Jose|     Men's Clothing|214.05|      Amex|\n",
      "|2012-01-01|09:00|Pittsburgh|       Pet Supplies|493.51|  Discover|\n",
      "|2012-01-01|09:00|     Omaha|Children's Clothing|235.63|MasterCard|\n",
      "+----------+-----+----------+-------------------+------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+-----+----------+-------------------+------+----------+\n",
      "|      Date| Time|      City|               Item| Total|   Payment|\n",
      "+----------+-----+----------+-------------------+------+----------+\n",
      "|2012-01-01|09:00|  San Jose|     Men's Clothing|214.05|      Amex|\n",
      "|2012-01-01|09:00|Pittsburgh|       Pet Supplies|493.51|  Discover|\n",
      "|2012-01-01|09:00|     Omaha|Children's Clothing|235.63|MasterCard|\n",
      "+----------+-----+----------+-------------------+------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter hanya data baris yang nilai kolom \"Total\" > 200\n",
    "purchaseDataframe.filter(purchaseDataframe['Total'] > 200).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e7ce1b",
   "metadata": {},
   "source": [
    "## Menyortir dataFrame berdasarkan kolom tertentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2186a80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------------+------+----------+\n",
      "|      Date| Time|       City|            Item| Total|   Payment|\n",
      "+----------+-----+-----------+----------------+------+----------+\n",
      "|2012-10-07|11:11|Albuquerque|    Pet Supplies| 308.7|      Visa|\n",
      "|2012-10-07|11:40|Albuquerque|            Toys|299.63|MasterCard|\n",
      "|2012-10-07|11:13|Albuquerque|Women's Clothing|419.49|  Discover|\n",
      "|2012-10-07|10:39|Albuquerque|    Pet Supplies| 401.3|MasterCard|\n",
      "+----------+-----+-----------+----------------+------+----------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+----------+-----+-----------+----------------+------+----------+\n",
      "|      Date| Time|       City|            Item| Total|   Payment|\n",
      "+----------+-----+-----------+----------------+------+----------+\n",
      "|2012-10-07|11:11|Albuquerque|    Pet Supplies| 308.7|      Visa|\n",
      "|2012-10-07|11:40|Albuquerque|            Toys|299.63|MasterCard|\n",
      "|2012-10-07|11:13|Albuquerque|Women's Clothing|419.49|  Discover|\n",
      "|2012-10-07|10:39|Albuquerque|    Pet Supplies| 401.3|MasterCard|\n",
      "+----------+-----+-----------+----------------+------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sortedByCity = purchaseDataframe.orderBy('City').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7642a23",
   "metadata": {},
   "source": [
    "## Menghitung jumlah transaksi di setiap kota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d26d03a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           City|count|\n",
      "+---------------+-----+\n",
      "|North Las Vegas|40013|\n",
      "|        Phoenix|40333|\n",
      "|          Omaha|40209|\n",
      "|      Anchorage|39806|\n",
      "|        Anaheim|40086|\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|           City|count|\n",
      "+---------------+-----+\n",
      "|North Las Vegas|40013|\n",
      "|        Phoenix|40333|\n",
      "|          Omaha|40209|\n",
      "|      Anchorage|39806|\n",
      "|        Anaheim|40086|\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numTransactionEachCity = purchaseDataframe.groupBy(\"City\").count()\n",
    "numTransactionEachCity.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f176c5b",
   "metadata": {},
   "source": [
    "## Mengindeks dan Mengakses DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a09682f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------------------+------+----------+-----+\n",
      "|      Date| Time|      City|               Item| Total|   Payment|index|\n",
      "+----------+-----+----------+-------------------+------+----------+-----+\n",
      "|2012-01-01|09:00|  San Jose|     Men's Clothing|214.05|      Amex|    0|\n",
      "|2012-01-01|09:00|Fort Worth|   Women's Clothing|153.57|      Visa|    1|\n",
      "|2012-01-01|09:00| San Diego|              Music| 66.08|      Cash|    2|\n",
      "|2012-01-01|09:00|Pittsburgh|       Pet Supplies|493.51|  Discover|    3|\n",
      "|2012-01-01|09:00|     Omaha|Children's Clothing|235.63|MasterCard|    4|\n",
      "|2012-01-01|09:00|  Stockton|     Men's Clothing|247.18|MasterCard|    5|\n",
      "|2012-01-01|09:00|    Austin|            Cameras| 379.6|      Visa|    6|\n",
      "+----------+-----+----------+-------------------+------+----------+-----+\n",
      "only showing top 7 rows\n",
      "\n",
      "+----------+-----+----------+-------------------+------+----------+-----+\n",
      "|      Date| Time|      City|               Item| Total|   Payment|index|\n",
      "+----------+-----+----------+-------------------+------+----------+-----+\n",
      "|2012-01-01|09:00|  San Jose|     Men's Clothing|214.05|      Amex|    0|\n",
      "|2012-01-01|09:00|Fort Worth|   Women's Clothing|153.57|      Visa|    1|\n",
      "|2012-01-01|09:00| San Diego|              Music| 66.08|      Cash|    2|\n",
      "|2012-01-01|09:00|Pittsburgh|       Pet Supplies|493.51|  Discover|    3|\n",
      "|2012-01-01|09:00|     Omaha|Children's Clothing|235.63|MasterCard|    4|\n",
      "|2012-01-01|09:00|  Stockton|     Men's Clothing|247.18|MasterCard|    5|\n",
      "|2012-01-01|09:00|    Austin|            Cameras| 379.6|      Visa|    6|\n",
      "+----------+-----+----------+-------------------+------+----------+-----+\n",
      "only showing top 7 rows\n",
      "\n",
      "+----------+-----+----------+-------------------+------+----------+-----+\n",
      "|      Date| Time|      City|               Item| Total|   Payment|index|\n",
      "+----------+-----+----------+-------------------+------+----------+-----+\n",
      "|2012-01-01|09:00| San Diego|              Music| 66.08|      Cash|    2|\n",
      "|2012-01-01|09:00|Pittsburgh|       Pet Supplies|493.51|  Discover|    3|\n",
      "|2012-01-01|09:00|     Omaha|Children's Clothing|235.63|MasterCard|    4|\n",
      "+----------+-----+----------+-------------------+------+----------+-----+\n",
      "\n",
      "+----------+-----+----------+-------------------+------+----------+-----+\n",
      "|      Date| Time|      City|               Item| Total|   Payment|index|\n",
      "+----------+-----+----------+-------------------+------+----------+-----+\n",
      "|2012-01-01|09:00| San Diego|              Music| 66.08|      Cash|    2|\n",
      "|2012-01-01|09:00|Pittsburgh|       Pet Supplies|493.51|  Discover|    3|\n",
      "|2012-01-01|09:00|     Omaha|Children's Clothing|235.63|MasterCard|    4|\n",
      "+----------+-----+----------+-------------------+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mengimpor monotonically_increasing_id\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "newPurchasedDataframe = purchaseDataframe.withColumn(\n",
    "    \"index\", monotonically_increasing_id())\n",
    "newPurchasedDataframe.show(7)\n",
    "row2Till4 = newPurchasedDataframe.filter((newPurchasedDataframe['index']>=2) &\n",
    "                                         (newPurchasedDataframe['index']<=4))\n",
    "row2Till4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a429c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Total|\n",
      "+-----+\n",
      "|66.08|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|Total|\n",
      "+-----+\n",
      "|66.08|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataRow2ColumnTotal = newPurchasedDataframe.filter(newPurchasedDataframe['index']==2).select('Total')\n",
    "dataRow2ColumnTotal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c1d40",
   "metadata": {},
   "source": [
    "## Menggunakan kueri SQL di dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e572e1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "| Total|Payment|\n",
      "+------+-------+\n",
      "|214.05|   Amex|\n",
      "|153.57|   Visa|\n",
      "| 66.08|   Cash|\n",
      "+------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+-------+\n",
      "| Total|Payment|\n",
      "+------+-------+\n",
      "|214.05|   Amex|\n",
      "|153.57|   Visa|\n",
      "| 66.08|   Cash|\n",
      "+------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#kita perlu membuat tampilan sementara sql untuk dataFrame kita\n",
    "purchaseDataframe.createOrReplaceTempView(\"purchaseSql\")\n",
    "\n",
    "#pilih kolom \"Total\" dan \"Payment\" dari tampilan sementara sql\n",
    "anotherNewDataframe = spark.sql(\"SELECT Total, Payment FROM purchaseSql\")\n",
    "anotherNewDataframe.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5eae10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------------+------+----------+\n",
      "|      Date| Time|       City|            Item| Total|   Payment|\n",
      "+----------+-----+-----------+----------------+------+----------+\n",
      "|2012-10-07|11:11|Albuquerque|    Pet Supplies| 308.7|      Visa|\n",
      "|2012-10-07|11:41|Albuquerque|           Music|365.64|      Visa|\n",
      "|2012-10-07|11:13|Albuquerque|Women's Clothing|419.49|  Discover|\n",
      "|2012-10-07|10:39|Albuquerque|    Pet Supplies| 401.3|MasterCard|\n",
      "|2012-10-07|11:18|Albuquerque|          Crafts|475.77|      Visa|\n",
      "+----------+-----+-----------+----------------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-----+-----------+----------------+------+----------+\n",
      "|      Date| Time|       City|            Item| Total|   Payment|\n",
      "+----------+-----+-----------+----------------+------+----------+\n",
      "|2012-10-07|11:11|Albuquerque|    Pet Supplies| 308.7|      Visa|\n",
      "|2012-10-07|11:41|Albuquerque|           Music|365.64|      Visa|\n",
      "|2012-10-07|11:13|Albuquerque|Women's Clothing|419.49|  Discover|\n",
      "|2012-10-07|10:39|Albuquerque|    Pet Supplies| 401.3|MasterCard|\n",
      "|2012-10-07|11:18|Albuquerque|          Crafts|475.77|      Visa|\n",
      "+----------+-----+-----------+----------------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sorting data berdasarkan kolom \"City\" menurut abjad\n",
    "orderByCity = spark.sql(\"SELECT * FROM purchaseSql ORDER BY City\")\n",
    "orderByCity.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5da0c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------+-------------------+------+-------+\n",
      "|      Date| Time|          City|               Item| Total|Payment|\n",
      "+----------+-----+--------------+-------------------+------+-------+\n",
      "|2012-10-07|10:34|      Richmond|Children's Clothing|252.45|   Amex|\n",
      "|2012-10-07|10:36|San Bernardino|               Toys|272.91|   Amex|\n",
      "|2012-10-07|10:34|     Baltimore|              Books|299.94|   Amex|\n",
      "|2012-10-07|10:33|       Lincoln|       Pet Supplies|359.44|   Amex|\n",
      "+----------+-----+--------------+-------------------+------+-------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+----------+-----+--------------+-------------------+------+-------+\n",
      "|      Date| Time|          City|               Item| Total|Payment|\n",
      "+----------+-----+--------------+-------------------+------+-------+\n",
      "|2012-10-07|10:34|      Richmond|Children's Clothing|252.45|   Amex|\n",
      "|2012-10-07|10:36|San Bernardino|               Toys|272.91|   Amex|\n",
      "|2012-10-07|10:34|     Baltimore|              Books|299.94|   Amex|\n",
      "|2012-10-07|10:33|       Lincoln|       Pet Supplies|359.44|   Amex|\n",
      "+----------+-----+--------------+-------------------+------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter nilai kolom Total>50 dan urutkan berdasarkan cara pembayaran\n",
    "filterAndSortWithSQL = spark.sql(\"SELECT * FROM purchaseSql WHERE Total>200 ORDER BY Payment\")\n",
    "filterAndSortWithSQL.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69bbc6b",
   "metadata": {},
   "source": [
    "# Data Preprocessing in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43aa04",
   "metadata": {},
   "source": [
    "## Baca file data dan buat skema data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af33650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "#membuat session\n",
    "appName = \"data preprocessing in Spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "flightSchema = StructType([\n",
    "  StructField(\"DayofMonth\", IntegerType(), False),\n",
    "  StructField(\"DayOfWeek\", IntegerType(), False),\n",
    "  StructField(\"Carrier\", StringType(), False),\n",
    "  StructField(\"OriginAirportID\", IntegerType(), False),\n",
    "  StructField(\"DestAirportID\", IntegerType(), False),\n",
    "  StructField(\"DepDelay\", IntegerType(), False),\n",
    "  StructField(\"ArrDelay\", IntegerType(), False),\n",
    "])\n",
    "\n",
    "flights = spark.read.csv('dataset/raw-flight-data.csv', \n",
    "                         schema=flightSchema, header=True)\n",
    "flights.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92b29b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----+--------------------+\n",
      "|airport_id|       city|state|                name|\n",
      "+----------+-----------+-----+--------------------+\n",
      "|     10165|Adak Island|   AK|                Adak|\n",
      "|     10299|  Anchorage|   AK|Ted Stevens Ancho...|\n",
      "+----------+-----------+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+-----------+-----+--------------------+\n",
      "|airport_id|       city|state|                name|\n",
      "+----------+-----------+-----+--------------------+\n",
      "|     10165|Adak Island|   AK|                Adak|\n",
      "|     10299|  Anchorage|   AK|Ted Stevens Ancho...|\n",
      "+----------+-----------+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportSchema = StructType([\n",
    "  StructField(\"airport_id\", IntegerType(), False),\n",
    "  StructField(\"city\", StringType(), False),\n",
    "  StructField(\"state\", StringType(), False),\n",
    "  StructField(\"name\", StringType(), False),\n",
    "])\n",
    "\n",
    "airports = spark.read.csv('dataset/airports.csv', header=True, \n",
    "                          schema=airportSchema)\n",
    "airports.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9fd5a5",
   "metadata": {},
   "source": [
    "Gabungkan dua dataFrame (penerbangan dan bandara), dan tunjukkan berapa banyak penerbangan dari setiap Kota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5d58529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|          city|count|\n",
      "+--------------+-----+\n",
      "|       Phoenix|90281|\n",
      "|         Omaha|13537|\n",
      "|Raleigh/Durham|28436|\n",
      "+--------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------+-----+\n",
      "|          city|count|\n",
      "+--------------+-----+\n",
      "|       Phoenix|90281|\n",
      "|         Omaha|13537|\n",
      "|Raleigh/Durham|28436|\n",
      "+--------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsByOrigin = flights.join(airports,\n",
    "                               flights.OriginAirportID == \n",
    "                               airports.airport_id).groupBy(\"city\").count()\n",
    "flightsByOrigin.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa00880",
   "metadata": {},
   "source": [
    "## Menangani duplikasi data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f08a18",
   "metadata": {},
   "source": [
    "Hapus duplikasi data dan hitung berapa banyak data diduplikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e9a9d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of original data rows:  2719418\n",
      "number of original data rows:  2719418\n",
      "number of data rows after deleting duplicated data:  2696983\n",
      "number of duplicated data:  22435\n",
      "number of data rows after deleting duplicated data:  2696983\n",
      "number of duplicated data:  22435\n"
     ]
    }
   ],
   "source": [
    "#hitung jumlah baris data asli\n",
    "n1 = flights.count()\n",
    "print(\"number of original data rows: \", n1)\n",
    "#hitung jumlah baris data setelah menghapus data yang diduplikasi\n",
    "n2 = flights.dropDuplicates().count()\n",
    "print(\"number of data rows after deleting duplicated data: \", n2)\n",
    "n3 = n1 - n2\n",
    "print(\"number of duplicated data: \", n3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1320406",
   "metadata": {},
   "source": [
    "Kita juga dapat menentukan kriteria duplikat dengan kolom tertentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4ac30fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+\n",
      "|name|age|height|\n",
      "+----+---+------+\n",
      "|Rony| 27|   168|\n",
      "|Rony| 15|   165|\n",
      "|Rony| 27|   168|\n",
      "+----+---+------+\n",
      "\n",
      "+----+---+------+\n",
      "|name|age|height|\n",
      "+----+---+------+\n",
      "|Rony| 27|   168|\n",
      "|Rony| 15|   165|\n",
      "|Rony| 27|   168|\n",
      "+----+---+------+\n",
      "\n",
      "+----+---+------+\n",
      "|name|age|height|\n",
      "+----+---+------+\n",
      "|Rony| 27|   168|\n",
      "|Rony| 15|   165|\n",
      "+----+---+------+\n",
      "\n",
      "+----+---+------+\n",
      "|name|age|height|\n",
      "+----+---+------+\n",
      "|Rony| 27|   168|\n",
      "|Rony| 15|   165|\n",
      "+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"Rony\",27, 168), \n",
    "                            (\"Rony\",15, 165), \n",
    "                            (\"Rony\",27, 168)], \n",
    "                           [\"name\",\"age\",\"height\"])\n",
    "df.show()\n",
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b70a1440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+\n",
      "|name|age|height|\n",
      "+----+---+------+\n",
      "|Rony| 27|   168|\n",
      "+----+---+------+\n",
      "\n",
      "+----+---+------+\n",
      "|name|age|height|\n",
      "+----+---+------+\n",
      "|Rony| 27|   168|\n",
      "+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates(['name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4865f4",
   "metadata": {},
   "source": [
    "## Menangani data yang hilang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c50e24f",
   "metadata": {},
   "source": [
    "Hapus baris jika setidaknya ada satu (kolom) data yang hilang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "59045b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of missing value rows:  46233\n",
      "number of missing value rows:  46233\n"
     ]
    }
   ],
   "source": [
    "flightsNoMissingValue = flights.dropDuplicates().dropna(\n",
    "    how=\"any\", subset=[\"ArrDelay\", \"DepDelay\"])# gunakan how=\"all\" untuk semua kolom data yang hilang\n",
    "numberOfMissingValueAny = n1 - flightsNoMissingValue.count()\n",
    "print(\"number of missing value rows: \", numberOfMissingValueAny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4de420",
   "metadata": {},
   "source": [
    "Isi data yang hilang menggunakan nilai rata-rata dari setiap data kolom yang sesuai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "248becd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean ArrDelay:  6.63768791455498\n",
      "mean ArrDelay:  6.63768791455498\n",
      "mean DepDelay:  10.53686662649788\n",
      "mean DepDelay:  10.53686662649788\n",
      "+----------------+\n",
      "|   avg(ArrDelay)|\n",
      "+----------------+\n",
      "|6.63768791455498|\n",
      "+----------------+\n",
      "\n",
      "+----------------+\n",
      "|   avg(ArrDelay)|\n",
      "+----------------+\n",
      "|6.63768791455498|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ambil nilai rata-rata\n",
    "meanArrDelay = flights.groupBy().avg(\"ArrDelay\").take(1)[0][0]\n",
    "print(\"mean ArrDelay: \", meanArrDelay)\n",
    "meanDepDelay = flights.groupBy().avg(\"DepDelay\").take(1)[0][0]\n",
    "print(\"mean DepDelay: \", meanDepDelay)\n",
    "#hapus data yang digandakan dan isi data yang hilang dengan nilai rata-rata\n",
    "flightsCleanData=flights.fillna(\n",
    "    {'ArrDelay': meanArrDelay, 'DepDelay': meanDepDelay})\n",
    "#hanya untuk percobaan\n",
    "flights.groupBy().avg(\"ArrDelay\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a5b9d8",
   "metadata": {},
   "source": [
    "## Jelajahi statistik data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4fc42a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|          DepDelay|         ArrDelay|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|           2719418|          2719418|\n",
      "|   mean|10.531448640848888|6.630879842672218|\n",
      "| stddev| 35.91695039008075|38.44200618946938|\n",
      "|    min|               -63|              -94|\n",
      "|    max|              1863|             1845|\n",
      "+-------+------------------+-----------------+\n",
      "\n",
      "+-------+------------------+-----------------+\n",
      "|summary|          DepDelay|         ArrDelay|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|           2719418|          2719418|\n",
      "|   mean|10.531448640848888|6.630879842672218|\n",
      "| stddev| 35.91695039008075|38.44200618946938|\n",
      "|    min|               -63|              -94|\n",
      "|    max|              1863|             1845|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsCleanData.describe('DepDelay','ArrDelay').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d28d250",
   "metadata": {},
   "source": [
    "Kita juga dapat menghitung korelasi antara dua variabel untuk mengetahui apakah variabel tersebut berhubungan satu sama lain atau tidak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b022ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation between departure delay and arrival delay:  0.939353821557276\n",
      "correlation between departure delay and arrival delay:  0.939353821557276\n"
     ]
    }
   ],
   "source": [
    "correlation = flightsCleanData.corr('DepDelay', 'ArrDelay')\n",
    "print(\"correlation between departure delay and arrival delay: \", \n",
    "      correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694a8fd",
   "metadata": {},
   "source": [
    "# Regression in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0161cb5",
   "metadata": {},
   "source": [
    "## Meningimpor modul dan membuat sesi spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "569beaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#create Spark session\n",
    "appName = \"Regression in Spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf461ec0",
   "metadata": {},
   "source": [
    "## Membaca data file kedalam dataframe spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6012ef78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define our schema\n",
    "flightSchema = StructType([\n",
    "  StructField(\"DayofMonth\", IntegerType(), False),\n",
    "  StructField(\"DayOfWeek\", IntegerType(), False),\n",
    "  StructField(\"Carrier\", StringType(), False),\n",
    "  StructField(\"OriginAirportID\", IntegerType(), False),\n",
    "  StructField(\"DestAirportID\", IntegerType(), False),\n",
    "  StructField(\"DepDelay\", IntegerType(), False),\n",
    "  StructField(\"ArrDelay\", IntegerType(), False),\n",
    "])\n",
    "#read csv data with our defined schema\n",
    "flightDataFrame = spark.read.csv('dataset/flights.csv', schema=flightSchema, header=True)\n",
    "flightDataFrame.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980f1a7",
   "metadata": {},
   "source": [
    "## Memilih data atau fitur penting untuk regresi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c48fa672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "|        19|        5|          11433|        13303|      -3|       1|\n",
      "|        19|        5|          14869|        12478|       0|      -8|\n",
      "|        19|        5|          14057|        14869|      -4|     -15|\n",
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "|        19|        5|          11433|        13303|      -3|       1|\n",
      "|        19|        5|          14869|        12478|       0|      -8|\n",
      "|        19|        5|          14057|        14869|      -4|     -15|\n",
      "+----------+---------+---------------+-------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select related column data for our regression input features\n",
    "data = flightDataFrame.select(\"DayofMonth\", \"DayOfWeek\", \n",
    "                              \"OriginAirportID\", \"DestAirportID\", \n",
    "                              \"DepDelay\", \"ArrDelay\")\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137497e",
   "metadata": {},
   "source": [
    "## Membagi data kedalam data latih dan data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2437f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows:  1891330 ; Testing data rows: 810888\n",
      "Training data rows:  1891330 ; Testing data rows: 810888\n"
     ]
    }
   ],
   "source": [
    "#membagi data, 70% training, 30% testing\n",
    "dividedData = data.randomSplit([0.7, 0.3])\n",
    "trainingData = dividedData[0] #index 0 = data training\n",
    "testingData = dividedData[1] #index 1 = data testing\n",
    "train_rows = trainingData.count()\n",
    "test_rows = testingData.count()\n",
    "print(\"Training data rows: \", train_rows, \"; Testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf53c3",
   "metadata": {},
   "source": [
    "## Mempersiapkan data latih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9190bcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+\n",
      "|features                      |label|\n",
      "+------------------------------+-----+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|-11  |\n",
      "|[1.0,1.0,10140.0,10397.0,-2.0]|-17  |\n",
      "|[1.0,1.0,10140.0,10821.0,8.0] |-9   |\n",
      "+------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------------------------+-----+\n",
      "|features                      |label|\n",
      "+------------------------------+-----+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|-11  |\n",
      "|[1.0,1.0,10140.0,10397.0,-2.0]|-17  |\n",
      "|[1.0,1.0,10140.0,10821.0,8.0] |-9   |\n",
      "+------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define an assembler\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    \"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \n",
    "    \"DepDelay\"], outputCol=\"features\")\n",
    "\n",
    "#change our features into one column using our defined assembler\n",
    "trainingDataFinal = assembler.transform(trainingData).select(\n",
    "    col(\"features\"), (col(\"ArrDelay\").cast(\"Int\").alias(\"label\")))\n",
    "trainingDataFinal.show(truncate=False , n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e768ac0",
   "metadata": {},
   "source": [
    "## Melatih model regresi menggunakan data latih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9fdf1f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression model is trained!\n",
      "Regression model is trained!\n"
     ]
    }
   ],
   "source": [
    "#call Spark linear regression we import before\n",
    "algoritma = LinearRegression(\n",
    "    labelCol=\"label\",featuresCol=\"features\", \n",
    "    maxIter=10, regParam=0.3)\n",
    "#train the model\n",
    "model = algoritma.fit(trainingDataFinal)\n",
    "print (\"Regression model is trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d47182",
   "metadata": {},
   "source": [
    "# Mempersiapkan data testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2c2ee5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------+\n",
      "|features                      |trueLabel|\n",
      "+------------------------------+---------+\n",
      "|[1.0,1.0,10140.0,11259.0,-2.0]|-14      |\n",
      "|[1.0,1.0,10140.0,11259.0,-1.0]|-11      |\n",
      "+------------------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------------------------+---------+\n",
      "|features                      |trueLabel|\n",
      "+------------------------------+---------+\n",
      "|[1.0,1.0,10140.0,11259.0,-2.0]|-14      |\n",
      "|[1.0,1.0,10140.0,11259.0,-1.0]|-11      |\n",
      "+------------------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#change our feature data into one column using our defined assembler\n",
    "#just like what we did before in the training data\n",
    "testingDataFinal = assembler.transform(\n",
    "    testingData).select(\n",
    "    col(\"features\"), (col(\"ArrDelay\")).cast(\"Int\").alias(\"trueLabel\"))\n",
    "testingDataFinal.show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f2486",
   "metadata": {},
   "source": [
    "## Memprediksi data testing menggunakan model yang telah dilatih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fa5a9c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------------+\n",
      "|            features|trueLabel|         prediction|\n",
      "+--------------------+---------+-------------------+\n",
      "|[1.0,1.0,10140.0,...|      -14| -5.758745937056345|\n",
      "|[1.0,1.0,10140.0,...|      -11| -4.761671320056825|\n",
      "|[1.0,1.0,10140.0,...|      -12|-3.7645967030573058|\n",
      "+--------------------+---------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+---------+-------------------+\n",
      "|            features|trueLabel|         prediction|\n",
      "+--------------------+---------+-------------------+\n",
      "|[1.0,1.0,10140.0,...|      -14| -5.758745937056345|\n",
      "|[1.0,1.0,10140.0,...|      -11| -4.761671320056825|\n",
      "|[1.0,1.0,10140.0,...|      -12|-3.7645967030573058|\n",
      "+--------------------+---------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predict testing data using our model\n",
    "prediction = model.transform(testingDataFinal)\n",
    "#show some prediction results\n",
    "prediction.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e1e0ef",
   "metadata": {},
   "source": [
    "## Menghitung performa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "86e838a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): 13.183545001214409\n",
      "Root Mean Square Error (RMSE): 13.183545001214409\n"
     ]
    }
   ],
   "source": [
    "#import evaluator module for regression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#define our evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "#calculate RMSE of our trained model\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print (\"Root Mean Square Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09635d",
   "metadata": {},
   "source": [
    "# Classification in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45320ce8",
   "metadata": {},
   "source": [
    "## Mengimpor modul dan membuat sesi spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "deb4bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#create Spark session\n",
    "appName = \"Classification in Spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d27bbb9",
   "metadata": {},
   "source": [
    "## Membaca data file kedalam dataframe spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d736c766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define our schema\n",
    "flightSchema = StructType([\n",
    "  StructField(\"DayofMonth\", IntegerType(), False),\n",
    "  StructField(\"DayOfWeek\", IntegerType(), False),\n",
    "  StructField(\"Carrier\", StringType(), False),\n",
    "  StructField(\"OriginAirportID\", IntegerType(), False),\n",
    "  StructField(\"DestAirportID\", IntegerType(), False),\n",
    "  StructField(\"DepDelay\", IntegerType(), False),\n",
    "  StructField(\"ArrDelay\", IntegerType(), False),\n",
    "])\n",
    "\n",
    "#read csv data with our defined schema\n",
    "csv = spark.read.csv('dataset/flights.csv', schema=flightSchema, header=True)\n",
    "csv.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2caa7cf",
   "metadata": {},
   "source": [
    "## Memilih data penting atau fitur data untuk klasifikasi dan mengubah fitur arrival delay ke binary class \"late\" vs \"not late\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d99b349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+-------------+--------+----+\n",
      "|DayofMonth|DayOfWeek|OriginAirportID|DestAirportID|DepDelay|Late|\n",
      "+----------+---------+---------------+-------------+--------+----+\n",
      "|        19|        5|          11433|        13303|      -3|   0|\n",
      "|        19|        5|          14869|        12478|       0|   0|\n",
      "|        19|        5|          14057|        14869|      -4|   0|\n",
      "+----------+---------+---------------+-------------+--------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+---------+---------------+-------------+--------+----+\n",
      "|DayofMonth|DayOfWeek|OriginAirportID|DestAirportID|DepDelay|Late|\n",
      "+----------+---------+---------------+-------------+--------+----+\n",
      "|        19|        5|          11433|        13303|      -3|   0|\n",
      "|        19|        5|          14869|        12478|       0|   0|\n",
      "|        19|        5|          14057|        14869|      -4|   0|\n",
      "+----------+---------+---------------+-------------+--------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = csv.select(\n",
    "    \"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \n",
    "    \"DepDelay\", ((col(\"ArrDelay\") > 15).cast(\"Int\").alias(\"Late\")))\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657a82c",
   "metadata": {},
   "source": [
    "## Membagi data kedalam data latih dan data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a163b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows:  1891093 ; Testing data rows: 811125\n",
      "Training data rows:  1891093 ; Testing data rows: 811125\n"
     ]
    }
   ],
   "source": [
    "#membagi data, 70% training, 30% testing\n",
    "dividedData = data.randomSplit([0.7, 0.3])\n",
    "trainingData = dividedData[0] #index 0 = data training\n",
    "testingData = dividedData[1] #index 1 = data testing\n",
    "train_rows = trainingData.count()\n",
    "test_rows = testingData.count()\n",
    "print(\"Training data rows: \", train_rows, \"; Testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d8cc4",
   "metadata": {},
   "source": [
    "## Menyipkan data latih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df59156e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+\n",
      "|features                      |label|\n",
      "+------------------------------+-----+\n",
      "|[1.0,1.0,10140.0,10821.0,8.0] |0    |\n",
      "|[1.0,1.0,10140.0,11259.0,-3.0]|0    |\n",
      "+------------------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------------------------+-----+\n",
      "|features                      |label|\n",
      "+------------------------------+-----+\n",
      "|[1.0,1.0,10140.0,10821.0,8.0] |0    |\n",
      "|[1.0,1.0,10140.0,11259.0,-3.0]|0    |\n",
      "+------------------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define an assembler\n",
    "assembler = VectorAssembler(inputCols = [\n",
    "    \"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \n",
    "    \"DepDelay\"], outputCol=\"features\")\n",
    "trainingDataFinal = assembler.transform(\n",
    "    trainingData).select(col(\"features\"), col(\"Late\").alias(\"label\"))\n",
    "trainingDataFinal.show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90081ffd",
   "metadata": {},
   "source": [
    "## Melatih model klasifikasi menggunakan data latih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d0905843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier model is trained!\n",
      "Classifier model is trained!\n"
     ]
    }
   ],
   "source": [
    "#define our classifier\n",
    "classifier = LogisticRegression(\n",
    "    labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3)\n",
    "#train our classifier\n",
    "model = classifier.fit(trainingDataFinal)\n",
    "print (\"Classifier model is trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e041439",
   "metadata": {},
   "source": [
    "## Mempersiapkan data testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8cadec99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|trueLabel|\n",
      "+--------------------+---------+\n",
      "|[1.0,1.0,10140.0,...|        0|\n",
      "|[1.0,1.0,10140.0,...|        0|\n",
      "|[1.0,1.0,10140.0,...|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+---------+\n",
      "|            features|trueLabel|\n",
      "+--------------------+---------+\n",
      "|[1.0,1.0,10140.0,...|        0|\n",
      "|[1.0,1.0,10140.0,...|        0|\n",
      "|[1.0,1.0,10140.0,...|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testingDataFinal = assembler.transform(\n",
    "    testingData).select(col(\"features\"), col(\"Late\").alias(\"trueLabel\"))\n",
    "testingDataFinal.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08528d",
   "metadata": {},
   "source": [
    "## Memprediksi data testing menggunakan model yang telah dilatih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "13893e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "|features                      |prediction|probability                             |trueLabel|\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|0.0       |[0.83140579153588,0.16859420846412]     |0        |\n",
      "|[1.0,1.0,10140.0,10397.0,-2.0]|0.0       |[0.8274504924386975,0.17254950756130247]|0        |\n",
      "|[1.0,1.0,10140.0,11259.0,-2.0]|0.0       |[0.8278173299459717,0.1721826700540283] |0        |\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "|features                      |prediction|probability                             |trueLabel|\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|0.0       |[0.83140579153588,0.16859420846412]     |0        |\n",
      "|[1.0,1.0,10140.0,10397.0,-2.0]|0.0       |[0.8274504924386975,0.17254950756130247]|0        |\n",
      "|[1.0,1.0,10140.0,11259.0,-2.0]|0.0       |[0.8278173299459717,0.1721826700540283] |0        |\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "|features                      |trueLabel|rawPrediction                           |probability                             |prediction|\n",
      "+------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|0        |[1.5956232985673249,-1.5956232985673249]|[0.83140579153588,0.16859420846412]     |0.0       |\n",
      "|[1.0,1.0,10140.0,10397.0,-2.0]|0        |[1.5676650819427487,-1.5676650819427487]|[0.8274504924386975,0.17254950756130247]|0.0       |\n",
      "|[1.0,1.0,10140.0,11259.0,-2.0]|0        |[1.5702365655877981,-1.5702365655877981]|[0.8278173299459717,0.1721826700540283] |0.0       |\n",
      "+------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "|features                      |trueLabel|rawPrediction                           |probability                             |prediction|\n",
      "+------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|0        |[1.5956232985673249,-1.5956232985673249]|[0.83140579153588,0.16859420846412]     |0.0       |\n",
      "|[1.0,1.0,10140.0,10397.0,-2.0]|0        |[1.5676650819427487,-1.5676650819427487]|[0.8274504924386975,0.17254950756130247]|0.0       |\n",
      "|[1.0,1.0,10140.0,11259.0,-2.0]|0        |[1.5702365655877981,-1.5702365655877981]|[0.8278173299459717,0.1721826700540283] |0.0       |\n",
      "+------------------------------+---------+----------------------------------------+----------------------------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(testingDataFinal)\n",
    "predictionFinal = prediction.select(\n",
    "    \"features\", \"prediction\", \"probability\", \"trueLabel\")\n",
    "predictionFinal.show(truncate=False, n=3)\n",
    "prediction.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ecc2d1",
   "metadata": {},
   "source": [
    "## Menghitung performa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "685a97b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 668288 , total data: 811125 , accuracy: 0.8239026044074588\n",
      "correct prediction: 668288 , total data: 811125 , accuracy: 0.8239026044074588\n"
     ]
    }
   ],
   "source": [
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['trueLabel']).count()\n",
    "totalData = predictionFinal.count()\n",
    "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData, \n",
    "      \", accuracy:\", correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c030ac",
   "metadata": {},
   "source": [
    "## Mencoba algoritma klasifikasi lain yang disediakan Spark (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eb0c9381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is trained!\n",
      "Model is trained!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "model2 = RandomForestClassifier(\n",
    "    numTrees=3, maxDepth=5, seed=42, labelCol=\"label\",featuresCol=\"features\")\n",
    "model2 = model2.fit(trainingDataFinal)\n",
    "print (\"Model is trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "95641595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "|features                      |prediction|probability                             |trueLabel|\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|0.0       |[0.9300913633017692,0.06990863669823082]|0        |\n",
      "|[1.0,1.0,10140.0,10397.0,-2.0]|0.0       |[0.9300913633017692,0.06990863669823082]|0        |\n",
      "|[1.0,1.0,10140.0,11259.0,-2.0]|0.0       |[0.9300913633017692,0.06990863669823082]|0        |\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "|features                      |prediction|probability                             |trueLabel|\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "|[1.0,1.0,10140.0,10397.0,-4.0]|0.0       |[0.9300913633017692,0.06990863669823082]|0        |\n",
      "|[1.0,1.0,10140.0,10397.0,-2.0]|0.0       |[0.9300913633017692,0.06990863669823082]|0        |\n",
      "|[1.0,1.0,10140.0,11259.0,-2.0]|0.0       |[0.9300913633017692,0.06990863669823082]|0        |\n",
      "+------------------------------+----------+----------------------------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "correct prediction: 751515 , total data: 811125 , accuracy 0.9265094775774387\n",
      "correct prediction: 751515 , total data: 811125 , accuracy 0.9265094775774387\n"
     ]
    }
   ],
   "source": [
    "prediction = model2.transform(testingDataFinal)\n",
    "predictionFinal = prediction.select(\n",
    "    \"features\", \"prediction\", \"probability\", \"trueLabel\")\n",
    "predictionFinal.show(truncate=False, n=3)\n",
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['trueLabel']).count()\n",
    "totalData = predictionFinal.count()\n",
    "print(\"correct prediction:\", correctPrediction, \", total data:\", \n",
    "      totalData, \", accuracy\", correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9604d1",
   "metadata": {},
   "source": [
    "# Sentiment Analysis In Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793950e",
   "metadata": {},
   "source": [
    "## Mengimpor modul dan membuat sesi spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7fa49033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
    "\n",
    "#create Spark session\n",
    "appName = \"Sentiment Analysis in Spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4055fcf4",
   "metadata": {},
   "source": [
    "## Membaca data file kedalam dataframe spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f6936131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+---------------------------------+\n",
      "|ItemID|Sentiment|SentimentSource|SentimentText                    |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "|1038  |1        |Sentiment140   |that film is fantastic #brilliant|\n",
      "|1804  |1        |Sentiment140   |this music is really bad #myband |\n",
      "|1693  |0        |Sentiment140   |winter is terrible #thumbs-down  |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+---------+---------------+---------------------------------+\n",
      "|ItemID|Sentiment|SentimentSource|SentimentText                    |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "|1038  |1        |Sentiment140   |that film is fantastic #brilliant|\n",
      "|1804  |1        |Sentiment140   |this music is really bad #myband |\n",
      "|1693  |0        |Sentiment140   |winter is terrible #thumbs-down  |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv file into dataFrame with automatically inferred schema\n",
    "tweets_csv = spark.read.csv('dataset/tweets.csv', inferSchema=True, header=True)\n",
    "tweets_csv.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ece2974",
   "metadata": {},
   "source": [
    "## Memilih data yang berhubungan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b31e5914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+\n",
      "|SentimentText                    |label|\n",
      "+---------------------------------+-----+\n",
      "|that film is fantastic #brilliant|1    |\n",
      "|this music is really bad #myband |1    |\n",
      "|winter is terrible #thumbs-down  |0    |\n",
      "|this game is awful #nightmare    |0    |\n",
      "|I love jam #loveit               |1    |\n",
      "+---------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------------------------+-----+\n",
      "|SentimentText                    |label|\n",
      "+---------------------------------+-----+\n",
      "|that film is fantastic #brilliant|1    |\n",
      "|this music is really bad #myband |1    |\n",
      "|winter is terrible #thumbs-down  |0    |\n",
      "|this game is awful #nightmare    |0    |\n",
      "|I love jam #loveit               |1    |\n",
      "+---------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select only \"SentimentText\" and \"Sentiment\" column, \n",
    "#and cast \"Sentiment\" column data into integer\n",
    "data = tweets_csv.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
    "data.show(truncate = False,n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2740e62",
   "metadata": {},
   "source": [
    "## Membagi data kedalam data latih dan data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c74035ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows: 1370 ; Testing data rows: 562\n",
      "Training data rows: 1370 ; Testing data rows: 562\n"
     ]
    }
   ],
   "source": [
    "#divide data, 70% for training, 30% for testing\n",
    "dividedData = data.randomSplit([0.7, 0.3]) \n",
    "trainingData = dividedData[0] #index 0 = data training\n",
    "testingData = dividedData[1] #index 1 = data testing\n",
    "train_rows = trainingData.count()\n",
    "test_rows = testingData.count()\n",
    "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326c4a0e",
   "metadata": {},
   "source": [
    "## Menyipkan data latih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4c5266e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----+------------------------------+\n",
      "|SentimentText            |label|SentimentWords                |\n",
      "+-------------------------+-----+------------------------------+\n",
      "|I adore cheese #bestever |1    |[i, adore, cheese, #bestever] |\n",
      "|I adore cheese #brilliant|1    |[i, adore, cheese, #brilliant]|\n",
      "|I adore cheese #favorite |1    |[i, adore, cheese, #favorite] |\n",
      "|I adore cheese #loveit   |1    |[i, adore, cheese, #loveit]   |\n",
      "|I adore cheese #thumbs-up|1    |[i, adore, cheese, #thumbs-up]|\n",
      "+-------------------------+-----+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------------+-----+------------------------------+\n",
      "|SentimentText            |label|SentimentWords                |\n",
      "+-------------------------+-----+------------------------------+\n",
      "|I adore cheese #bestever |1    |[i, adore, cheese, #bestever] |\n",
      "|I adore cheese #brilliant|1    |[i, adore, cheese, #brilliant]|\n",
      "|I adore cheese #favorite |1    |[i, adore, cheese, #favorite] |\n",
      "|I adore cheese #loveit   |1    |[i, adore, cheese, #loveit]   |\n",
      "|I adore cheese #thumbs-up|1    |[i, adore, cheese, #thumbs-up]|\n",
      "+-------------------------+-----+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"SentimentText\", outputCol=\"SentimentWords\")\n",
    "tokenizedTrain = tokenizer.transform(trainingData)\n",
    "tokenizedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28bd472",
   "metadata": {},
   "source": [
    "### Menghapus stop words (kata yang tidak penting menjadi fitur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3817ea1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "|SentimentText            |label|SentimentWords                |MeaningfulWords            |\n",
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "|I adore cheese #bestever |1    |[i, adore, cheese, #bestever] |[adore, cheese, #bestever] |\n",
      "|I adore cheese #brilliant|1    |[i, adore, cheese, #brilliant]|[adore, cheese, #brilliant]|\n",
      "|I adore cheese #favorite |1    |[i, adore, cheese, #favorite] |[adore, cheese, #favorite] |\n",
      "|I adore cheese #loveit   |1    |[i, adore, cheese, #loveit]   |[adore, cheese, #loveit]   |\n",
      "|I adore cheese #thumbs-up|1    |[i, adore, cheese, #thumbs-up]|[adore, cheese, #thumbs-up]|\n",
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "|SentimentText            |label|SentimentWords                |MeaningfulWords            |\n",
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "|I adore cheese #bestever |1    |[i, adore, cheese, #bestever] |[adore, cheese, #bestever] |\n",
      "|I adore cheese #brilliant|1    |[i, adore, cheese, #brilliant]|[adore, cheese, #brilliant]|\n",
      "|I adore cheese #favorite |1    |[i, adore, cheese, #favorite] |[adore, cheese, #favorite] |\n",
      "|I adore cheese #loveit   |1    |[i, adore, cheese, #loveit]   |[adore, cheese, #loveit]   |\n",
      "|I adore cheese #thumbs-up|1    |[i, adore, cheese, #thumbs-up]|[adore, cheese, #thumbs-up]|\n",
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
    "SwRemovedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c667dc84",
   "metadata": {},
   "source": [
    "### Mengkonversi fitur kata ke fitur numerikal. Di spark 2.2.1, diimplementasikan dalam fungsi HashinhgTF menggunkan Austin Appleby's MurmurHash 3 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d500b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------+-------------------------------------------+\n",
      "|label|MeaningfulWords            |features                                   |\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "|1    |[adore, cheese, #bestever] |(262144,[1689,91011,100089],[1.0,1.0,1.0]) |\n",
      "|1    |[adore, cheese, #brilliant]|(262144,[1689,45361,100089],[1.0,1.0,1.0]) |\n",
      "|1    |[adore, cheese, #favorite] |(262144,[1689,100089,108624],[1.0,1.0,1.0])|\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "|label|MeaningfulWords            |features                                   |\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "|1    |[adore, cheese, #bestever] |(262144,[1689,91011,100089],[1.0,1.0,1.0]) |\n",
      "|1    |[adore, cheese, #brilliant]|(262144,[1689,45361,100089],[1.0,1.0,1.0]) |\n",
      "|1    |[adore, cheese, #favorite] |(262144,[1689,100089,108624],[1.0,1.0,1.0])|\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "numericTrainData = hashTF.transform(SwRemovedTrain).select(\n",
    "    'label', 'MeaningfulWords', 'features')\n",
    "numericTrainData.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e07a3",
   "metadata": {},
   "source": [
    "## Melatih model klasifikasi menggunakan data latih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "214e1251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n",
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", \n",
    "                        maxIter=10, regParam=0.01)\n",
    "model = lr.fit(numericTrainData)\n",
    "print (\"Training is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd60a9",
   "metadata": {},
   "source": [
    "## Mempersiapkan data testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "452395f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------+-------------------------------------------------------+\n",
      "|Label|MeaningfulWords                      |features                                               |\n",
      "+-----+-------------------------------------+-------------------------------------------------------+\n",
      "|1    |[adore, classical, music, #bestever] |(262144,[91011,100089,102383,131250],[1.0,1.0,1.0,1.0])|\n",
      "|1    |[adore, classical, music, #thumbs-up]|(262144,[88825,100089,102383,131250],[1.0,1.0,1.0,1.0])|\n",
      "+-----+-------------------------------------+-------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----+-------------------------------------+-------------------------------------------------------+\n",
      "|Label|MeaningfulWords                      |features                                               |\n",
      "+-----+-------------------------------------+-------------------------------------------------------+\n",
      "|1    |[adore, classical, music, #bestever] |(262144,[91011,100089,102383,131250],[1.0,1.0,1.0,1.0])|\n",
      "|1    |[adore, classical, music, #thumbs-up]|(262144,[88825,100089,102383,131250],[1.0,1.0,1.0,1.0])|\n",
      "+-----+-------------------------------------+-------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizedTest = tokenizer.transform(testingData)\n",
    "SwRemovedTest = swr.transform(tokenizedTest)\n",
    "numericTest = hashTF.transform(SwRemovedTest).select(\n",
    "    'Label', 'MeaningfulWords', 'features')\n",
    "numericTest.show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb09e7",
   "metadata": {},
   "source": [
    "## Memprediksi data testing dan menghitung akurasi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4b072221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+----------+-----+\n",
      "|MeaningfulWords                      |prediction|Label|\n",
      "+-------------------------------------+----------+-----+\n",
      "|[adore, classical, music, #bestever] |1.0       |1    |\n",
      "|[adore, classical, music, #thumbs-up]|1.0       |1    |\n",
      "|[adore, coffee, #bestever]           |1.0       |1    |\n",
      "|[adore, coffee, #thumbs-up]          |1.0       |1    |\n",
      "+-------------------------------------+----------+-----+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-------------------------------------+----------+-----+\n",
      "|MeaningfulWords                      |prediction|Label|\n",
      "+-------------------------------------+----------+-----+\n",
      "|[adore, classical, music, #bestever] |1.0       |1    |\n",
      "|[adore, classical, music, #thumbs-up]|1.0       |1    |\n",
      "|[adore, coffee, #bestever]           |1.0       |1    |\n",
      "|[adore, coffee, #thumbs-up]          |1.0       |1    |\n",
      "+-------------------------------------+----------+-----+\n",
      "only showing top 4 rows\n",
      "\n",
      "correct prediction: 554 , total data: 562 , accuracy: 0.9857651245551602\n",
      "correct prediction: 554 , total data: 562 , accuracy: 0.9857651245551602\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(numericTest)\n",
    "predictionFinal = prediction.select(\n",
    "    \"MeaningfulWords\", \"prediction\", \"Label\")\n",
    "predictionFinal.show(n=4, truncate = False)\n",
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
    "totalData = predictionFinal.count()\n",
    "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData, \n",
    "      \", accuracy:\", correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd2374",
   "metadata": {},
   "source": [
    "# Clustering in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05676d2d",
   "metadata": {},
   "source": [
    "## Mengimpor modul dan membuat sesi spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b92a1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#create session\n",
    "appName = \"Clustering in Spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c5b18",
   "metadata": {},
   "source": [
    "## Membaca data file kedalam dataframe spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ccf4a8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+-------------+-----------+------+-------------+--------------+---------+----------+---------+----+\n",
      "|   CustomerName|Age|MaritalStatus|IncomeRange|Gender|TotalChildren|ChildrenAtHome|Education|Occupation|HomeOwner|Cars|\n",
      "+---------------+---+-------------+-----------+------+-------------+--------------+---------+----------+---------+----+\n",
      "|    Aaron Adams| 42|            0|      50000|     0|            0|             0|        3|         2|        1|   1|\n",
      "|Aaron Alexander| 40|            1|      50000|     0|            0|             0|        2|         2|        1|   2|\n",
      "|    Aaron Allen| 63|            0|      25000|     0|            2|             1|        2|         1|        1|   2|\n",
      "+---------------+---+-------------+-----------+------+-------------+--------------+---------+----------+---------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------------+---+-------------+-----------+------+-------------+--------------+---------+----------+---------+----+\n",
      "|   CustomerName|Age|MaritalStatus|IncomeRange|Gender|TotalChildren|ChildrenAtHome|Education|Occupation|HomeOwner|Cars|\n",
      "+---------------+---+-------------+-----------+------+-------------+--------------+---------+----------+---------+----+\n",
      "|    Aaron Adams| 42|            0|      50000|     0|            0|             0|        3|         2|        1|   1|\n",
      "|Aaron Alexander| 40|            1|      50000|     0|            0|             0|        2|         2|        1|   2|\n",
      "|    Aaron Allen| 63|            0|      25000|     0|            2|             1|        2|         1|        1|   2|\n",
      "+---------------+---+-------------+-----------+------+-------------+--------------+---------+----------+---------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv file using automatically inferred schema\n",
    "customers = spark.read.csv(\n",
    "    'dataset/customers.csv', inferSchema=True, header=True)\n",
    "customers.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28574d3d",
   "metadata": {},
   "source": [
    "Description for each column data:\n",
    "\n",
    "- CustomerName: name of customer\n",
    "- Age: age of customer (in year)\n",
    "- MaritalStatus: (1=married, 0=not married)\n",
    "- IncomeRange: income per year (in USD)\n",
    "- Gender: (1=female, 2=male)\n",
    "- TotalChildren: number of children customer has\n",
    "- ChildrenAtHome: number of children living with customer (in the same home)\n",
    "- Education: (1=high school, 2=bachelor, 3=master, 4=PhD, 5=Post-doc)\n",
    "- Occupation: (0=unskilled manual work until 5=professional)\n",
    "- HomeOwner: (1=owning a home, 0=not owning a home)\n",
    "- Cars: number of car customer has"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ab0ee",
   "metadata": {},
   "source": [
    "## Mempersiapkan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "29ed0ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------------------------------+\n",
      "|CustomerName   |features                                      |\n",
      "+---------------+----------------------------------------------+\n",
      "|Aaron Adams    |[42.0,0.0,50000.0,0.0,0.0,0.0,3.0,2.0,1.0,1.0]|\n",
      "|Aaron Alexander|[40.0,1.0,50000.0,0.0,0.0,0.0,2.0,2.0,1.0,2.0]|\n",
      "|Aaron Allen    |[63.0,0.0,25000.0,0.0,2.0,1.0,2.0,1.0,1.0,2.0]|\n",
      "+---------------+----------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------------+----------------------------------------------+\n",
      "|CustomerName   |features                                      |\n",
      "+---------------+----------------------------------------------+\n",
      "|Aaron Adams    |[42.0,0.0,50000.0,0.0,0.0,0.0,3.0,2.0,1.0,1.0]|\n",
      "|Aaron Alexander|[40.0,1.0,50000.0,0.0,0.0,0.0,2.0,2.0,1.0,2.0]|\n",
      "|Aaron Allen    |[63.0,0.0,25000.0,0.0,2.0,1.0,2.0,1.0,1.0,2.0]|\n",
      "+---------------+----------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define assembler\n",
    "assembler = VectorAssembler(inputCols = [\n",
    "    \"Age\", \"MaritalStatus\", \"IncomeRange\", \"Gender\", \"TotalChildren\", \n",
    "    \"ChildrenAtHome\", \"Education\", \"Occupation\", \"HomeOwner\", \"Cars\"], \n",
    "                            outputCol=\"features\")\n",
    "data = assembler.transform(customers).select('CustomerName', 'features')\n",
    "data.show(truncate = False, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab5789",
   "metadata": {},
   "source": [
    "## Membuat Model Clustering K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3a66689c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is successfully trained!\n",
      "Model is successfully trained!\n"
     ]
    }
   ],
   "source": [
    "#define kMeans clustering algorithm\n",
    "kmeans = KMeans(\n",
    "    featuresCol=assembler.getOutputCol(), \n",
    "    predictionCol=\"cluster\", k=5)\n",
    "model = kmeans.fit(data)\n",
    "print (\"Model is successfully trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7bf3f6",
   "metadata": {},
   "source": [
    "## Mencetak centroid pada setiap cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0ebf9368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[5.19737441e+01 5.26868545e-01 5.00000000e+04 4.93961141e-01\n",
      " 1.34552774e+00 4.98337126e-01 3.23035183e+00 2.77927534e+00\n",
      " 6.62699107e-01 1.14615789e+00]\n",
      "[5.82794840e+01 6.22850123e-01 1.50000000e+05 4.79729730e-01\n",
      " 2.07248157e+00 3.20638821e+00 3.41461916e+00 4.34705160e+00\n",
      " 6.48648649e-01 3.10995086e+00]\n",
      "[5.53417813e+01 5.72411296e-01 1.00000000e+05 4.97103548e-01\n",
      " 2.54380883e+00 1.54272266e+00 3.46198407e+00 4.19116582e+00\n",
      " 7.16509776e-01 1.94532947e+00]\n",
      "[5.60711289e+01 5.83804487e-01 7.50000000e+04 5.03921211e-01\n",
      " 2.17308043e+00 8.16706183e-01 3.73244574e+00 3.92759438e+00\n",
      " 7.23326646e-01 1.38063104e+00]\n",
      "[5.31013005e+01 4.17180014e-01 2.50000000e+04 4.80492813e-01\n",
      " 1.41512663e+00 6.08487337e-01 2.31622177e+00 1.45448323e+00\n",
      " 5.93086927e-01 1.11464750e+00]\n",
      "Cluster Centers: \n",
      "[5.19737441e+01 5.26868545e-01 5.00000000e+04 4.93961141e-01\n",
      " 1.34552774e+00 4.98337126e-01 3.23035183e+00 2.77927534e+00\n",
      " 6.62699107e-01 1.14615789e+00]\n",
      "[5.82794840e+01 6.22850123e-01 1.50000000e+05 4.79729730e-01\n",
      " 2.07248157e+00 3.20638821e+00 3.41461916e+00 4.34705160e+00\n",
      " 6.48648649e-01 3.10995086e+00]\n",
      "[5.53417813e+01 5.72411296e-01 1.00000000e+05 4.97103548e-01\n",
      " 2.54380883e+00 1.54272266e+00 3.46198407e+00 4.19116582e+00\n",
      " 7.16509776e-01 1.94532947e+00]\n",
      "[5.60711289e+01 5.83804487e-01 7.50000000e+04 5.03921211e-01\n",
      " 2.17308043e+00 8.16706183e-01 3.73244574e+00 3.92759438e+00\n",
      " 7.23326646e-01 1.38063104e+00]\n",
      "[5.31013005e+01 4.17180014e-01 2.50000000e+04 4.80492813e-01\n",
      " 1.41512663e+00 6.08487337e-01 2.31622177e+00 1.45448323e+00\n",
      " 5.93086927e-01 1.11464750e+00]\n"
     ]
    }
   ],
   "source": [
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfbfcf",
   "metadata": {},
   "source": [
    "## Cluster The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "98213b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|cluster|count|\n",
      "+-------+-----+\n",
      "|      0| 5713|\n",
      "|      1| 1628|\n",
      "|      2| 2762|\n",
      "|      3| 5483|\n",
      "|      4| 2922|\n",
      "+-------+-----+\n",
      "\n",
      "+---------------+-------+\n",
      "|   CustomerName|cluster|\n",
      "+---------------+-------+\n",
      "|    Aaron Adams|      0|\n",
      "|Aaron Alexander|      0|\n",
      "|    Aaron Allen|      4|\n",
      "|    Aaron Baker|      0|\n",
      "|   Aaron Bryant|      3|\n",
      "+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-----+\n",
      "|cluster|count|\n",
      "+-------+-----+\n",
      "|      0| 5713|\n",
      "|      1| 1628|\n",
      "|      2| 2762|\n",
      "|      3| 5483|\n",
      "|      4| 2922|\n",
      "+-------+-----+\n",
      "\n",
      "+---------------+-------+\n",
      "|   CustomerName|cluster|\n",
      "+---------------+-------+\n",
      "|    Aaron Adams|      0|\n",
      "|Aaron Alexander|      0|\n",
      "|    Aaron Allen|      4|\n",
      "|    Aaron Baker|      0|\n",
      "|   Aaron Bryant|      3|\n",
      "+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(data)#cluster given data\n",
    "prediction.groupBy(\"cluster\").count().orderBy(\"cluster\").show()#count members in each cluster\n",
    "prediction.select('CustomerName', 'cluster').show(5)#show several cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e568b9",
   "metadata": {},
   "source": [
    "# Recommender System in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f95d328",
   "metadata": {},
   "source": [
    "## Mengimpor modul dan membuat sesi spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "303ad8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import module\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "#create session\n",
    "appName = \"Recommender system in Spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d24334",
   "metadata": {},
   "source": [
    "## Membaca data file kedalam dataframe spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "342ab99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|movieId|userId|rating| timestamp|               title|              genres|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|     31|     1|   2.5|1260759144|Dangerous Minds (...|               Drama|\n",
      "|   1029|     1|   3.0|1260759179|        Dumbo (1941)|Animation|Childre...|\n",
      "|   1061|     1|   3.0|1260759182|     Sleepers (1996)|            Thriller|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|movieId|userId|rating| timestamp|               title|              genres|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|     31|     1|   2.5|1260759144|Dangerous Minds (...|               Drama|\n",
      "|   1029|     1|   3.0|1260759179|        Dumbo (1941)|Animation|Childre...|\n",
      "|   1061|     1|   3.0|1260759182|     Sleepers (1996)|            Thriller|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read file into dataFrame using automatically inferred schema\n",
    "ratings = spark.read.csv('dataset/ratings.csv', inferSchema=True, header=True)\n",
    "movies = spark.read.csv('dataset/movies.csv', inferSchema=True, header=True)\n",
    "#merge \"movies\" and \"ratings\" dataFrame based on \"movieId\"\n",
    "ratings.join(movies, \"movieId\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e107bb",
   "metadata": {},
   "source": [
    "## Persiapan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2de31c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data rows: 69982 , number of testing data rows: 30022\n",
      "number of training data rows: 69982 , number of testing data rows: 30022\n"
     ]
    }
   ],
   "source": [
    "#use only column data of \"userId\", \"movieId\", dan \"rating\"\n",
    "data = ratings.select(\"userId\", \"movieId\", \"rating\")\n",
    "#divide data, 70% for training and 30% for testing\n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0].withColumnRenamed(\"rating\", \"label\")\n",
    "test = splits[1].withColumnRenamed(\"rating\", \"trueLabel\")\n",
    "#calculate number of rows\n",
    "train_rows = train.count()\n",
    "test_rows = test.count()\n",
    "print (\"number of training data rows:\", train_rows, \n",
    "       \", number of testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f012069",
   "metadata": {},
   "source": [
    "## Mendefinisikan model dan melatihnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "801d7f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n",
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "#define ALS (Alternating Least Square) as our recommender system\n",
    "als = ALS(maxIter=19, regParam=0.01, userCol=\"userId\", \n",
    "          itemCol=\"movieId\", ratingCol=\"label\")\n",
    "#train our ALS model\n",
    "model = als.fit(train)\n",
    "print(\"Training is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e515733b",
   "metadata": {},
   "source": [
    "## Memprediksi data testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4c39e447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing is done!\n",
      "testing is done!\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(test)\n",
    "print(\"testing is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3e873b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------+----------+---------+\n",
      "|userId|title                      |prediction|trueLabel|\n",
      "+------+---------------------------+----------+---------+\n",
      "|232   |Guilty as Sin (1993)       |4.088933  |4.0      |\n",
      "|126   |Hudsucker Proxy, The (1994)|3.645697  |5.0      |\n",
      "|491   |Hudsucker Proxy, The (1994)|5.2743235 |3.0      |\n",
      "|452   |Hudsucker Proxy, The (1994)|4.543952  |3.0      |\n",
      "|92    |Hudsucker Proxy, The (1994)|2.0869389 |4.0      |\n",
      "|309   |Hudsucker Proxy, The (1994)|5.1569223 |4.0      |\n",
      "|607   |Hudsucker Proxy, The (1994)|4.127407  |4.0      |\n",
      "|15    |Hudsucker Proxy, The (1994)|2.9285796 |3.0      |\n",
      "|659   |Hudsucker Proxy, The (1994)|4.0024414 |4.0      |\n",
      "|241   |Hudsucker Proxy, The (1994)|3.702203  |4.0      |\n",
      "+------+---------------------------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+---------------------------+----------+---------+\n",
      "|userId|title                      |prediction|trueLabel|\n",
      "+------+---------------------------+----------+---------+\n",
      "|232   |Guilty as Sin (1993)       |4.088933  |4.0      |\n",
      "|126   |Hudsucker Proxy, The (1994)|3.645697  |5.0      |\n",
      "|491   |Hudsucker Proxy, The (1994)|5.2743235 |3.0      |\n",
      "|452   |Hudsucker Proxy, The (1994)|4.543952  |3.0      |\n",
      "|92    |Hudsucker Proxy, The (1994)|2.0869389 |4.0      |\n",
      "|309   |Hudsucker Proxy, The (1994)|5.1569223 |4.0      |\n",
      "|607   |Hudsucker Proxy, The (1994)|4.127407  |4.0      |\n",
      "|15    |Hudsucker Proxy, The (1994)|2.9285796 |3.0      |\n",
      "|659   |Hudsucker Proxy, The (1994)|4.0024414 |4.0      |\n",
      "|241   |Hudsucker Proxy, The (1994)|3.702203  |4.0      |\n",
      "+------+---------------------------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.join(movies, \"movieId\").select(\n",
    "    \"userId\", \"title\", \"prediction\", \"trueLabel\").show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bed5ee",
   "metadata": {},
   "source": [
    "## Mengevaluasi akurasi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d1d0c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): nan\n",
      "Root Mean Square Error (RMSE): nan\n"
     ]
    }
   ],
   "source": [
    "#import RegressionEvaluator since we also want to calculate RMSE (Root Mean Square Error)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print (\"Root Mean Square Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d8383a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of original data rows:  30022\n",
      "number of original data rows:  30022\n",
      "number of rows after dropping data with missing value:  28769\n",
      "number of missing data:  1253\n",
      "number of rows after dropping data with missing value:  28769\n",
      "number of missing data:  1253\n"
     ]
    }
   ],
   "source": [
    "prediction.count()\n",
    "a = prediction.count()\n",
    "print(\"number of original data rows: \", a)\n",
    "#drop rows with any missing data\n",
    "cleanPred = prediction.dropna(how=\"any\", subset=[\"prediction\"])\n",
    "b = cleanPred.count()\n",
    "print(\"number of rows after dropping data with missing value: \", b)\n",
    "print(\"number of missing data: \", a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a7e2d689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): 1.2582324528766913\n",
      "Root Mean Square Error (RMSE): 1.2582324528766913\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(cleanPred)\n",
    "print (\"Root Mean Square Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89020798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
